{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2eyDCKHmt6X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wikipedia import DisambiguationError, PageError\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCGkQegpmt6d"
   },
   "outputs": [],
   "source": [
    "def download_content(title, ln=100):\n",
    "    try:\n",
    "        content = wikipedia.page(title).content\n",
    "        content = content[:content.find('==')].strip()\n",
    "    except (DisambiguationError, PageError) as e:\n",
    "        return None, None\n",
    "    if len(content.split()) >= ln:\n",
    "        return title, content\n",
    "    return None, None\n",
    "\n",
    "def downloader(k=50, pool_size=10):\n",
    "    pages_fetch = {}\n",
    "    complete = False\n",
    "    with mp.Pool(pool_size) as pool:\n",
    "        while not complete:\n",
    "            titles = wikipedia.random(k)\n",
    "            res = pool.map_async(download_content, titles)\n",
    "            page = res.get()\n",
    "            for title, content in page:\n",
    "                pages_fetch[title] = content\n",
    "                if len(pages_fetch) > k:\n",
    "                    complete = True\n",
    "                    break\n",
    "    del pages_fetch[None]\n",
    "    \n",
    "    return pages_fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4yvYQST5mt6g",
    "outputId": "8f87a543-9c34-4f87-cbc9-3828c207d494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 wiki content downloaded in 5373.620978355408\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "n_wiki = 10000\n",
    "wiki_pickle = 'wiki_content.pkl'\n",
    "if wiki_pickle not in os.listdir():\n",
    "    page_dict = downloader(k=n_wiki)\n",
    "    with open(wiki_pickle, 'wb') as f:\n",
    "        pickle.dump(page_dict, f)\n",
    "else:\n",
    "    with open(wiki_pickle, 'rb') as f:\n",
    "        page_dict = pickle.load(f)\n",
    "print(f'{n_wiki} wiki content downloaded in {time.time() - t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvo8kEFQmt6l"
   },
   "outputs": [],
   "source": [
    "def tokenizer(sent):\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    tokens = [w for w in tokens if w not in string.punctuation]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = list(map(stemmer.stem, tokens))\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = list(map(lmtzr.lemmatize, tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "ox202tSmmt6o",
    "outputId": "048cc0c8-6943-42f5-c673-ee4f684c6f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for dataset of size 100.\n",
      "Recall@2: 1.0\n",
      "Recall@5: 1.0\n",
      "Recall@10: 1.0\n",
      "\n",
      "Recall score for dataset of size 200.\n",
      "Recall@2: 0.985\n",
      "Recall@5: 1.0\n",
      "Recall@10: 1.0\n",
      "\n",
      "Recall score for dataset of size 400.\n",
      "Recall@2: 0.985\n",
      "Recall@5: 0.9975\n",
      "Recall@10: 1.0\n",
      "\n",
      "Recall score for dataset of size 800.\n",
      "Recall@2: 0.9625\n",
      "Recall@5: 0.9925\n",
      "Recall@10: 0.99625\n",
      "\n",
      "Recall score for dataset of size 1600.\n",
      "Recall@2: 0.931875\n",
      "Recall@5: 0.978125\n",
      "Recall@10: 0.99\n",
      "\n",
      "Recall score for dataset of size 3200.\n",
      "Recall@2: 0.889375\n",
      "Recall@5: 0.9603125\n",
      "Recall@10: 0.981875\n",
      "\n",
      "Recall score for dataset of size 6400.\n",
      "Recall@2: 0.825625\n",
      "Recall@5: 0.921875\n",
      "Recall@10: 0.95859375\n",
      "\n",
      "Recall score for dataset of size 10000.\n",
      "Recall@2: 0.7844\n",
      "Recall@5: 0.8843\n",
      "Recall@10: 0.9392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pred(vec, X, q):\n",
    "    y = vec.transform([q])\n",
    "    res = np.dot(y, X.T).todense()\n",
    "    res = np.asarray(res).flatten()\n",
    "    res = np.argsort(res, axis=0)[::-1]\n",
    "    return res\n",
    "\n",
    "def evaluate(df, vec, X):\n",
    "    y_true = []\n",
    "    y_test = []\n",
    "    queries = []\n",
    "    for i, row in df.iterrows():\n",
    "        random.shuffle(row.top10pct)\n",
    "        queries.append(' '.join(row.top10pct[:5]))\n",
    "        y_true.append(i)\n",
    "        y_test.append(pred(vec, X, queries[-1]))\n",
    "    group_size = [2, 5, 10]\n",
    "    recall_k = dict(zip(group_size, [0] * len(group_size)))\n",
    "    for i in range(len(queries)):\n",
    "        for gs in group_size:\n",
    "            recall_k[gs] += 1 if y_true[i] in y_test[i][:gs] else 0\n",
    "    for gs in group_size:\n",
    "        recall_k[gs] /= len(df)\n",
    "    return recall_k\n",
    "\n",
    "def search_helper(df, vec, X, query, k=5):\n",
    "    ids = pred(vec, X, query)[:5]\n",
    "    res = []\n",
    "    for i in ids:\n",
    "        res.append((df.iloc[i].title, df.iloc[i].content))\n",
    "    return res\n",
    "\n",
    "def main():\n",
    "    main_df = pd.DataFrame(list(page_dict.items()), columns=['title', 'content'])\n",
    "    main_df['top10pct'] = None\n",
    "    for i, row in main_df.iterrows():\n",
    "        tokens = word_tokenize(row.content.lower())\n",
    "        stopwords_eng = stopwords.words('english')\n",
    "        tokens = [w for w in tokens if not (w in stopwords_eng or w in string.punctuation)]\n",
    "        freq = Counter(tokens)\n",
    "        top10 = sorted(freq.items(), key=lambda x: -x[1])[:int(len(freq) * 0.3)]\n",
    "        row.top10pct = [w for w, v in top10]\n",
    "    \n",
    "    i = 100\n",
    "\n",
    "    while i <= len(main_df):\n",
    "        df = main_df[:i]\n",
    "        vec = TfidfVectorizer(tokenizer=tokenizer)\n",
    "        X = vec.fit_transform(df.content)\n",
    "        recall_k = evaluate(df, vec, X)\n",
    "        print(f'Recall score for dataset of size {len(df)}.')\n",
    "        for gs, score in recall_k.items():\n",
    "            print(f'Recall@{gs}: {score}')\n",
    "        print()\n",
    "        i *= 2\n",
    "        if i > len(main_df) and len(df) < len(main_df):\n",
    "            i = len(main_df)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSMFnrJLmt6s"
   },
   "outputs": [],
   "source": [
    "def search():\n",
    "    main_df = pd.DataFrame(list(page_dict.items()), columns=['title', 'content'])\n",
    "    main_df['top10pct'] = None\n",
    "    for i, row in main_df.iterrows():\n",
    "        tokens = word_tokenize(row.content.lower())\n",
    "        stopwords_eng = stopwords.words('english')\n",
    "        tokens = [w for w in tokens if not (w in stopwords_eng or w in string.punctuation)]\n",
    "        freq = Counter(tokens)\n",
    "        top10 = sorted(freq.items(), key=lambda x: -x[1])[:int(len(freq) * 0.3)]\n",
    "        row.top10pct = [w for w, v in top10]\n",
    "    min_res = int(input('Input mininum number of results: '))\n",
    "    query = input('> ')\n",
    "    while query:\n",
    "        ids = pred(vec, X, query)[:min_res]\n",
    "        for i in ids:\n",
    "            print(f'{df.iloc[i].title}\\n\\n{df.iloc[i].content}\\n\\n\\n')\n",
    "        query = input('> ')\n",
    "# search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxSrEbI4mt6w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP-Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
